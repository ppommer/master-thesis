{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interface - *neutralizing-bias*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run in Shell\n",
    "__Concurrent__\n",
    "\n",
    "`python joint/inference.py --bert_encoder --bert_full_embeddings --coverage --debias_checkpoint models/debias_model.ckpt --debias_weight 1.3 --inference_output inference_concurrent/output.txt --no_tok_enrich --pointer_generator --test inference_concurrent/data.txt --working_dir inference_concurrent/`\n",
    "\n",
    "__Modular__\n",
    "\n",
    "`python joint/inference.py --activation_hidden --bert_full_embeddings --checkpoint models/model.ckpt --coverage --debias_weight 1.3 --extra_features_top --inference_output inference_modular/output.txt --pointer_generator --pre_enrich --test inference_modular/data.txt --token_softmax --working_dir inference_modular/`\n",
    "\n",
    "_Optional:_ `--categories_file bias_data/WNC/revision_topics.csv --category_input --tagging_pretrain_epochs 3 --test_batch_size 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pp/master-thesis/neutralizing-bias/.venv-nb/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "06/19/2022 13:42:18 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import subprocess\n",
    "import spacy\n",
    "from typing import Tuple, List, Dict, Union\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "\n",
    "NLP = spacy.load(\n",
    "    \"en_core_web_sm\"\n",
    ")  # run \"python -m spacy download en_core_web_sm\" to initially download the model\n",
    "TOKENIZER = BertTokenizer.from_pretrained(\"bert-base-uncased\",\n",
    "                                          cache_dir=\"cache\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_dep(s: str) -> Tuple[str, str]:\n",
    "    \"\"\"Get POS and dependency tags for a given string.\n",
    "    \"\"\"\n",
    "    toks = s.split()\n",
    "\n",
    "    def words_from_toks(toks):\n",
    "        words = []\n",
    "        word_indices = []\n",
    "        for i, tok in enumerate(toks):\n",
    "            if tok.startswith('##'):\n",
    "                words[-1] += tok.replace('##', '')\n",
    "                word_indices[-1].append(i)\n",
    "            else:\n",
    "                words.append(tok)\n",
    "                word_indices.append([i])\n",
    "        return words, word_indices\n",
    "\n",
    "    out_pos, out_dep = [], []\n",
    "    words, word_indices = words_from_toks(toks)\n",
    "    analysis = NLP(\" \".join(words))\n",
    "\n",
    "    if len(analysis) != len(words):\n",
    "        return None, None\n",
    "\n",
    "    for analysis_tok, idx in zip(analysis, word_indices):\n",
    "        out_pos += [analysis_tok.pos_] * len(idx)\n",
    "        out_dep += [analysis_tok.dep_] * len(idx)\n",
    "\n",
    "    assert len(out_pos) == len(out_dep) == len(toks)\n",
    "\n",
    "    return \" \".join(out_pos), \" \".join(out_dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s: str):\n",
    "    \"\"\"BERT-tokenize a given string.\n",
    "    \"\"\"\n",
    "    global TOKENIZER\n",
    "    tok_list = TOKENIZER.tokenize(s.strip())\n",
    "    return \" \".join(tok_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debias(\n",
    "    input: List[str],\n",
    "    concurrent: bool = False,\n",
    ") -> List[Dict[str, Union[str, float]]]:\n",
    "    \"\"\"\n",
    "    Adapter for running the de-biasing script \"inference.py\" on the concurrent and the modular model architecture.\n",
    "\n",
    "    Args:\n",
    "        input - list of sentences to be debiased\n",
    "        concurrent - run concurrent model (if False, run modular model)\n",
    "\n",
    "    Returns:\n",
    "        results - list of output dicts\n",
    "    \"\"\"\n",
    "    assert len(input) > 0\n",
    "\n",
    "    working_dir = \"inference_concurrent\" if concurrent else \"inference_modular\"\n",
    "    inference_output = os.path.join(working_dir, \"output.txt\")\n",
    "    test = os.path.join(working_dir, \"data.txt\")\n",
    "\n",
    "    if os.path.exists(inference_output):\n",
    "        os.remove(inference_output)\n",
    "\n",
    "    if os.path.exists(test):\n",
    "        os.remove(test)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # create input file\n",
    "    with open(test, \"w\") as f:\n",
    "        for s in input:\n",
    "            tok = tokenize(s)\n",
    "            pos, dep = get_pos_dep(tok)\n",
    "            f.write(\"0\\t\" + (tok + \"\\t\") * 2 + (s + \"\\t\") * 2 + pos + \"\\t\" +\n",
    "                    dep + \"\\n\")\n",
    "\n",
    "    # compile command line arguments\n",
    "    args = [\n",
    "        sys.executable, \"joint/inference.py\", \"--bert_full_embeddings\",\n",
    "        \"--coverage\", \"--debias_weight\", \"1.3\", \"--inference_output\",\n",
    "        inference_output, \"--pointer_generator\", \"--test\", test,\n",
    "        \"--working_dir\", working_dir\n",
    "    ]\n",
    "\n",
    "    if concurrent:\n",
    "        args.append(\"--bert_encoder\")\n",
    "        args.append(\"--debias_checkpoint\")\n",
    "        args.append(\"models/debias_model.ckpt\")\n",
    "        args.append(\"--no_tok_enrich\")\n",
    "\n",
    "    else:\n",
    "        args.append(\"--activation_hidden\")\n",
    "        args.append(\"--checkpoint\")\n",
    "        args.append(\"models/model.ckpt\")\n",
    "        args.append(\"--extra_features_top\")\n",
    "        args.append(\"--pre_enrich\")\n",
    "        args.append(\"--test_batch_size\")\n",
    "        args.append(\"1\")\n",
    "        args.append(\"--token_softmax\")\n",
    "\n",
    "    # execute shell script\n",
    "    subprocess.run(\" \".join(args), shell=True)\n",
    "    \n",
    "    ## use for debugging ##\n",
    "    # debug_output = subprocess.run(\" \".join(args),\n",
    "    #                               text=True,\n",
    "    #                               capture_output=True)\n",
    "    # print(debug_output.stderr)\n",
    "    # print(debug_output.stdout)\n",
    "    #######################\n",
    "\n",
    "    # read output\n",
    "    with open(inference_output, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if re.match(r'^IN SEQ:', line):\n",
    "                in_seq_tok = line.split(\"\\t\")[1][3:-2]\n",
    "                in_seq = in_seq_tok.replace(\" ##\", \"\")\n",
    "                in_seq = re.sub(r'\\s([.,;?!\"])', r'\\1', in_seq)\n",
    "\n",
    "            if re.match(r'^PRED SEQ:', line):\n",
    "                pred_seq_tok = line.split(\"\\t\")[1][3:-2]\n",
    "                pred_seq = pred_seq_tok.replace(\" ##\", \"\")\n",
    "                pred_seq = re.sub(r'\\s([.,;?!\"])', r'\\1', pred_seq)\n",
    "\n",
    "            if re.match(r'^PRED DIST:', line):\n",
    "                pred_dist = line.split(\"\\t\")[1]\n",
    "                pred_dist = ast.literal_eval(pred_dist)\n",
    "\n",
    "                # create sequence with underlined changes\n",
    "                change_seq = in_seq_tok.split()\n",
    "                difs = list(\n",
    "                    set(in_seq_tok.split()) - set(pred_seq_tok.split()))\n",
    "\n",
    "                for dif in difs:\n",
    "                    idx = change_seq.index(dif)\n",
    "\n",
    "                    if pred_dist[idx] > 0.1:\n",
    "                        change_seq[idx] = \"\\033[4m{}\\033[0m\".format(\n",
    "                            change_seq[idx])\n",
    "\n",
    "                change_seq = \" \".join(change_seq)\n",
    "                change_seq = change_seq.replace(\" ##\", \"\")\n",
    "                change_seq = re.sub(r'\\s([.,;?!\"])', r'\\1', change_seq)\n",
    "\n",
    "                results.append({\n",
    "                    \"change_seq\": change_seq,\n",
    "                    \"in_seq\": in_seq,\n",
    "                    \"pred_seq\": pred_seq,\n",
    "                    \"in_seq_tok\": in_seq_tok,\n",
    "                    \"pred_seq_tok\": pred_seq_tok,\n",
    "                    \"pred_dist\": pred_dist,\n",
    "                })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Define the input sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    \"jewish history is the history of the jewish people, their religion, and culture, as it developed and interacted with other dominant peoples, religions and cultures.\",\n",
    "    \"Black people are always criminal.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/13/2022 10:47:40 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at inference_modular/cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "2it [00:00, 2638.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKIPPED  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pp/master-thesis/neutralizing-bias/.venv-nb/lib/python3.10/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "06/13/2022 10:47:42 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at inference_modular/cache/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "06/13/2022 10:47:42 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file inference_modular/cache/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpj09v6h8b\n",
      "06/13/2022 10:47:45 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "06/13/2022 10:47:49 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at inference_modular/cache/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "06/13/2022 10:47:49 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file inference_modular/cache/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmprd3k3pg0\n",
      "06/13/2022 10:47:52 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "06/13/2022 10:47:56 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForMultitaskWithFeaturesOnTop not initialized from pretrained model: ['tok_classifier.out.0.weight', 'tok_classifier.out.0.bias', 'tok_classifier.enricher.0.weight', 'tok_classifier.enricher.0.bias', 'cls_classifier.weight', 'cls_classifier.bias']\n",
      "06/13/2022 10:47:56 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForMultitaskWithFeaturesOnTop: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.gamma', 'cls.predictions.transform.LayerNorm.beta', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING FROM models/model.ckpt\n",
      "...DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:24<00:00, 12.27s/it]\n"
     ]
    }
   ],
   "source": [
    "results_modular = debias(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change lines 75 and 80 in `joint/inference.py` to toggle GPU/CPU use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/13/2022 11:06:45 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at inference_concurrent/cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "2it [00:00, 2556.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKIPPED  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pp/master-thesis/neutralizing-bias/.venv-nb/lib/python3.10/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "06/13/2022 11:06:47 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at inference_concurrent/cache/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "06/13/2022 11:06:47 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file inference_concurrent/cache/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpuzsk8ytu\n",
      "06/13/2022 11:06:50 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "06/13/2022 11:06:54 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at inference_concurrent/cache/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "06/13/2022 11:06:54 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file inference_concurrent/cache/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpvu8wk0hv\n",
      "06/13/2022 11:06:57 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "06/13/2022 11:07:01 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForMultitask not initialized from pretrained model: ['cls_classifier.weight', 'cls_classifier.bias', 'tok_classifier.weight', 'tok_classifier.bias']\n",
      "06/13/2022 11:07:01 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForMultitask: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.gamma', 'cls.predictions.transform.LayerNorm.beta', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING DEBIASER FROM models/debias_model.ckpt\n",
      "DONE.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:33<00:00, 33.97s/it]\n"
     ]
    }
   ],
   "source": [
    "results_concurrent = debias(inputs, concurrent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:   jewish history is the history of the jewish people, their religion, and culture, as it developed and interacted with other dominant peoples, religions and cultures.\n",
      "Changes: jewish history is the history of the jewish people, their religion, and culture, as it developed and interacted with other \u001b[4mdominant\u001b[0m peoples, religions and cultures.\n",
      "Output:  jewish history is the history of the jewish people, their religion, and culture, as it developed and interacted with other peoples, religions and cultures.\n",
      "Bias scores: jewish (0.015) history (0.000) is (0.014) the (0.001) history (0.002) of (0.000) the (0.001) jewish (0.130) people (0.014) , (0.000) their (0.202) religion (0.002) , (0.000) and (0.000) culture (0.000) , (0.000) as (0.000) it (0.000) developed (0.002) and (0.000) interact (0.002) ##ed (0.001) with (0.000) other (0.024) dominant (0.445) peoples (0.002) , (0.000) religions (0.000) and (0.000) cultures (0.000) . (0.000) \n",
      "\n",
      "Input:   black people are always criminal.\n",
      "Changes: black people are \u001b[4malways\u001b[0m \u001b[4mcriminal\u001b[0m.\n",
      "Output:  black people are.\n",
      "Bias scores: black (0.008) people (0.007) are (0.002) always (0.809) criminal (0.137) . (0.000) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for result in results_modular:\n",
    "    print(\"Input:   {}\".format(result[\"in_seq\"]))\n",
    "    print(\"Changes: {}\".format(result[\"change_seq\"]))\n",
    "    print(\"Output:  {}\".format(result[\"pred_seq\"]))\n",
    "    print(\"Bias scores: \" + (\"{} ({:.3f}) \" * len(result[\"pred_dist\"])).format(\n",
    "        *tuple([\n",
    "            tok\n",
    "            for dist in zip(result[\"in_seq_tok\"].split(), result[\"pred_dist\"])\n",
    "            for tok in dist\n",
    "        ])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:   jewish history is the history of the jewish people, their religion, and culture, as it developed and interacted with other dominant peoples, religions and cultures.\n",
      "Changes: jewish history is the history of the jewish people, their religion, and culture, as it developed and interacted with other dominant peoples, religions and cultures.\n",
      "Output:  jewish history is the history of the jewish people, their religion, and culture, as it developed and interacted with other dominant peoples, religions and cultures.\n",
      "\n",
      "Input:   black people are always criminal.\n",
      "Changes: black people are \u001b[4malways\u001b[0m criminal.\n",
      "Output:  black people are sometimes criminal.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for result in results_concurrent:\n",
    "    print(\"Input:   {}\".format(result[\"in_seq\"]))\n",
    "    print(\"Changes: {}\".format(result[\"change_seq\"]))\n",
    "    print(\"Output:  {}\".format(result[\"pred_seq\"]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x: float, start_col: Tuple[int]=(255, 255, 255), end_col: Tuple[int]=(250, 50, 50)) -> str:\n",
    "    \"\"\" Returns the HEX code for a color at position x ∈ [0; 1] within a color gradient of start_col and end_col.\n",
    "    \"\"\"\n",
    "    rgb = (\n",
    "        int((1 - x) * start_col[0] + x * end_col[0]),\n",
    "        int((1 - x) * start_col[1] + x * end_col[1]),\n",
    "        int((1 - x) * start_col[2] + x * end_col[2]))\n",
    "    return \"#%02x%02x%02x\" % rgb\n",
    "\n",
    "\n",
    "def print_results(results: List[Dict[str, Union[str, List[float]]]], out_file: str=\"\") -> None:\n",
    "    \"\"\" Prints the result tokens with a highlighted background according to the subjective bias probability.\n",
    "    \"\"\"\n",
    "    from IPython.display import display, HTML\n",
    "\n",
    "    tok_lists = [x[\"in_seq_tok\"] for x in results]\n",
    "    dist_lists = [x[\"pred_dist\"]for x in results]\n",
    "    html_list = []\n",
    "\n",
    "    for toks, dists in zip(tok_lists, dist_lists):\n",
    "        html_string = \"<div style='background-color:white;padding:10px;margin:-8px'>\"\n",
    "\n",
    "        for tok, dist in zip(toks.split(\" \"), dists):\n",
    "            html_string += \"<span style='color:black;background-color: \" + gradient(dist) + \"'>\" + tok + \"</span>\" + \" \"\n",
    "\n",
    "        html_string += \"</div>\"\n",
    "        html_list.append(HTML(html_string))\n",
    "\n",
    "    if len(out_file) > 0:\n",
    "        with open(out_file, \"w\") as f:\n",
    "            f.write(\"\\n\".join(html.data) + \"\\n\")\n",
    "\n",
    "    for html in html_list:\n",
    "        display(html)\n",
    "\n",
    "\n",
    "print_results(results_modular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(\n",
    "    in_file: str,\n",
    "    out_file: str,\n",
    "    html: bool=False,\n",
    "    html_file: str=None\n",
    "    ) -> None:\n",
    "    \"\"\" Generates an output and optionally an HTML file.\n",
    "    \"\"\"\n",
    "    # read input\n",
    "    results = []\n",
    "    with open(in_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            if re.match(r'^IN SEQ:', line):\n",
    "                in_seq_tok = line.split(\"\\t\")[1][3:-2]\n",
    "                in_seq = in_seq_tok.replace(\" ##\", \"\")\n",
    "                in_seq = re.sub(r'\\s([.,;?!\"])', r'\\1', in_seq)\n",
    "\n",
    "            if re.match(r'^PRED SEQ:', line):\n",
    "                pred_seq_tok = line.split(\"\\t\")[1][3:-2]\n",
    "                pred_seq = pred_seq_tok.replace(\" ##\", \"\")\n",
    "                pred_seq = re.sub(r'\\s([.,;?!\"])', r'\\1', pred_seq)\n",
    "\n",
    "            if re.match(r'^PRED DIST:', line):\n",
    "                pred_dist = line.split(\"\\t\")[1]\n",
    "                pred_dist = ast.literal_eval(pred_dist)\n",
    "\n",
    "                results.append({\n",
    "                    \"in_seq_tok\": in_seq_tok,\n",
    "                    \"pred_seq\": pred_seq,\n",
    "                    \"pred_dist\": pred_dist,\n",
    "                })\n",
    "\n",
    "    # write output\n",
    "    with open(out_file, \"w\") as f:\n",
    "        text = \"\\n\".join([result[\"pred_seq\"] for result in results]) + \"\\n\"\n",
    "        text = text.replace(\" ' \", \"'\")\n",
    "        text = text.replace(\" / \", \"/\")\n",
    "        text = text.replace(\" & \", \"&\")\n",
    "        text = text.replace(\" \\' \", \"'\")\n",
    "        text = text.replace(\" \\'\", \"'\")\n",
    "        text = text.replace(\"( \", \"(\")\n",
    "        text = text.replace(\" )\", \")\")\n",
    "        text = text.replace(\" :\", \":\")\n",
    "        f.write(text)\n",
    "\n",
    "    # write HTML output\n",
    "    if html:\n",
    "        html_list = []\n",
    "        tok_lists = [x[\"in_seq_tok\"] for x in results]\n",
    "        dist_lists = [x[\"pred_dist\"]for x in results]\n",
    "\n",
    "        for toks, dists in zip(tok_lists, dist_lists):\n",
    "            html_string = \"<div style='background-color:white;padding:10px;margin:-8px'>\"\n",
    "\n",
    "            for tok, dist in zip(toks.split(\" \"), dists):\n",
    "                html_string += \"<span style='color:black;background-color: \" + gradient(dist) + \"'>\" + tok + \"</span>\" + \" \"\n",
    "\n",
    "            html_string += \"</div>\"\n",
    "            html_list.append(html_string)\n",
    "\n",
    "        with open(html_file, \"w\") as f:\n",
    "            f.write(\"\\n\".join(html_list) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_output(\n",
    "    in_file=\"/home/pp/master-thesis/neutralizing-bias/src/inference/modular/results_modular.txt\",\n",
    "    out_file=\"/home/pp/master-thesis/neutralizing-bias/src/inference/modular/output_modular.txt\",\n",
    "    html=True,\n",
    "    html_file=\"/home/pp/master-thesis/neutralizing-bias/src/inference/output_modular.html\"\n",
    ")\n",
    "\n",
    "generate_output(\n",
    "    in_file=\"/home/pp/master-thesis/neutralizing-bias/src/inference/concurrent/results_concurrent.txt\",\n",
    "    out_file=\"/home/pp/master-thesis/neutralizing-bias/src/inference/concurrent/output_concurrent.txt\",\n",
    ")\n",
    "\n",
    "for i in range(1, 11):\n",
    "    generate_output(\n",
    "        in_file=\"/home/pp/master-thesis/neutralizing-bias/src/inference/concurrent/iter_10/results_concurrent_\" + str(i) + \".txt\",\n",
    "        out_file=\"/home/pp/master-thesis/neutralizing-bias/src/inference/concurrent/iter_10/output_concurrent_\" + str(i) + \".txt\",\n",
    "    )\n",
    "\n",
    "    generate_output(\n",
    "        in_file=\"/home/pp/master-thesis/neutralizing-bias/src/inference/modular/iter_10/results_modular_\" + str(i) + \".txt\",\n",
    "        out_file=\"/home/pp/master-thesis/neutralizing-bias/src/inference/modular/iter_10/output_modular_\" + str(i) + \".txt\",\n",
    "        html=True,\n",
    "        html_file=\"/home/pp/master-thesis/neutralizing-bias/src/inference/modular/iter_10/output_modular_\" + str(i) + \".html\"\n",
    "    )\n",
    "\n",
    "generate_output(\n",
    "    in_file=\"/home/pp/master-thesis/neutralizing-bias/src/inference/modular/strap_modular/results_strap_modular.txt\",\n",
    "    out_file=\"/home/pp/master-thesis/neutralizing-bias/src/inference/modular/strap_modular/output_strap_modular.txt\",\n",
    ")\n",
    "\n",
    "generate_output(\n",
    "    in_file=\"/home/pp/master-thesis/neutralizing-bias/src/inference/concurrent/strap_concurrent/results_strap_concurrent.txt\",\n",
    "    out_file=\"/home/pp/master-thesis/neutralizing-bias/src/inference/concurrent/strap_concurrent/output_strap_concurrent.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6da1900acec40547dccdf6ed82ff53d67318af5ad673ca75f174b699dab6a3b4"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('.venv-nb': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
