{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interface - *neutralizing-bias*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/05/2022 17:01:02 - INFO - pytorch_pretrained_bert.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache, downloading to C:\\Users\\PAULPO~1\\AppData\\Local\\Temp\\tmp5kdk10nr\n",
      "100%|██████████| 231508/231508 [00:00<00:00, 661157.96B/s]\n",
      "05/05/2022 17:01:03 - INFO - pytorch_pretrained_bert.file_utils -   copying C:\\Users\\PAULPO~1\\AppData\\Local\\Temp\\tmp5kdk10nr to cache at cache\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "05/05/2022 17:01:03 - INFO - pytorch_pretrained_bert.file_utils -   creating metadata file for cache\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "05/05/2022 17:01:03 - INFO - pytorch_pretrained_bert.file_utils -   removing temp file C:\\Users\\PAULPO~1\\AppData\\Local\\Temp\\tmp5kdk10nr\n",
      "05/05/2022 17:01:03 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at cache\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import subprocess\n",
    "import spacy\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "\n",
    "NLP = spacy.load(\n",
    "    \"en_core_web_sm\"\n",
    ")  # run \"python -m spacy download en_core_web_sm\" to initially download the model\n",
    "TOKENIZER = BertTokenizer.from_pretrained(\"bert-base-uncased\",\n",
    "                                          cache_dir=\"cache\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_dep(s: str) -> tuple[str, str]:\n",
    "    \"\"\"Get POS and dependency tags for a given string.\n",
    "    \"\"\"\n",
    "    toks = s.split()\n",
    "\n",
    "    def words_from_toks(toks):\n",
    "        words = []\n",
    "        word_indices = []\n",
    "        for i, tok in enumerate(toks):\n",
    "            if tok.startswith('##'):\n",
    "                words[-1] += tok.replace('##', '')\n",
    "                word_indices[-1].append(i)\n",
    "            else:\n",
    "                words.append(tok)\n",
    "                word_indices.append([i])\n",
    "        return words, word_indices\n",
    "\n",
    "    out_pos, out_dep = [], []\n",
    "    words, word_indices = words_from_toks(toks)\n",
    "    analysis = NLP(' '.join(words))\n",
    "\n",
    "    if len(analysis) != len(words):\n",
    "        return None, None\n",
    "\n",
    "    for analysis_tok, idx in zip(analysis, word_indices):\n",
    "        out_pos += [analysis_tok.pos_] * len(idx)\n",
    "        out_dep += [analysis_tok.dep_] * len(idx)\n",
    "\n",
    "    assert len(out_pos) == len(out_dep) == len(toks)\n",
    "\n",
    "    return ' '.join(out_pos), ' '.join(out_dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s: str):\n",
    "    \"\"\"BERT-tokenize a given string.\n",
    "    \"\"\"\n",
    "    global TOKENIZER\n",
    "    tok_list = TOKENIZER.tokenize(s.strip())\n",
    "    return \" \".join(tok_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debias(\n",
    "    input: list[str],\n",
    "    concurrent: bool = False,\n",
    ") -> list[dict[str, str | float]]:\n",
    "    \"\"\"\n",
    "    Adapter for running the de-biasing script \"inference.py\" on the concurrent and the modular model architecture.\n",
    "\n",
    "    Args:\n",
    "        input - list of sentences to be debiased\n",
    "        concurrent - run concurrent model (if False, run modular model)\n",
    "\n",
    "    Returns:\n",
    "        results - list of output dicts\n",
    "    \"\"\"\n",
    "    assert len(input) > 0\n",
    "\n",
    "    working_dir = \"inference_concurrent\" if concurrent else \"inference_modular\"\n",
    "    inference_output = os.path.join(working_dir, \"output.txt\")\n",
    "    test = os.path.join(working_dir, \"data.txt\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # create input file\n",
    "    with open(test, \"w\") as f:\n",
    "        for s in input:\n",
    "            tok = tokenize(s)\n",
    "            pos, dep = get_pos_dep(tok)\n",
    "            f.write(\"0\\t\" + (tok + \"\\t\") * 2 + (s + \"\\t\") * 2 + pos + \"\\t\" +\n",
    "                    dep + \"\\n\")\n",
    "\n",
    "    # compile command line arguments\n",
    "    args = [\n",
    "        sys.executable, \"joint/inference.py\", \"--bert_full_embeddings\",\n",
    "        \"--coverage\", \"--debias_weight\", \"1.3\", \"--inference_output\",\n",
    "        inference_output, \"--pointer_generator\", \"--test\", test,\n",
    "        \"--working_dir\", working_dir\n",
    "    ]\n",
    "\n",
    "    if concurrent:\n",
    "        args.append(\"--bert_encoder\")\n",
    "        args.append(\"--debias_checkpoint\")\n",
    "        args.append(\"debias_model.ckpt\")\n",
    "        args.append(\"--no_tok_enrich\")\n",
    "\n",
    "    else:\n",
    "        args.append(\"--activation_hidden\")\n",
    "        args.append(\"--checkpoint\")\n",
    "        args.append(\"model.ckpt\")\n",
    "        args.append(\"--extra_features_top\")\n",
    "        args.append(\"--pre_enrich\")\n",
    "        args.append(\"--test_batch_size\")\n",
    "        args.append(\"1\")\n",
    "        args.append(\"--token_softmax\")\n",
    "\n",
    "    # execute shell script\n",
    "    subprocess.run(\" \".join(args))\n",
    "    \n",
    "    ## use for debugging ##\n",
    "    # debug_output = subprocess.run(\" \".join(args),\n",
    "    #                               text=True,\n",
    "    #                               capture_output=True)\n",
    "    # print(debug_output.stderr)\n",
    "    # print(debug_output.stdout)\n",
    "    #######################\n",
    "\n",
    "    # read output\n",
    "    with open(inference_output, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if re.match(r'^IN SEQ:', line):\n",
    "                in_seq_tok = line.split(\"\\t\")[1][3:-2]\n",
    "                in_seq = in_seq_tok.replace(\" ##\", \"\")\n",
    "                in_seq = re.sub(r'\\s([.,;?!\"])', r'\\1', in_seq)\n",
    "\n",
    "            if re.match(r'^PRED SEQ:', line):\n",
    "                pred_seq_tok = line.split(\"\\t\")[1][3:-2]\n",
    "                pred_seq = pred_seq_tok.replace(\" ##\", \"\")\n",
    "                pred_seq = re.sub(r'\\s([.,;?!\"])', r'\\1', pred_seq)\n",
    "\n",
    "            if re.match(r'^PRED DIST:', line):\n",
    "                pred_dist = line.split(\"\\t\")[1]\n",
    "                pred_dist = ast.literal_eval(pred_dist)\n",
    "\n",
    "                # create sequence with underlined changes\n",
    "                change_seq = in_seq_tok.split()\n",
    "                difs = list(\n",
    "                    set(in_seq_tok.split()) - set(pred_seq_tok.split()))\n",
    "\n",
    "                for dif in difs:\n",
    "                    idx = change_seq.index(dif)\n",
    "\n",
    "                    if pred_dist[idx] > 0.1:\n",
    "                        change_seq[idx] = \"\\033[4m{}\\033[0m\".format(\n",
    "                            change_seq[idx])\n",
    "\n",
    "                change_seq = \" \".join(change_seq)\n",
    "                change_seq = change_seq.replace(\" ##\", \"\")\n",
    "                change_seq = re.sub(r'\\s([.,;?!\"])', r'\\1', change_seq)\n",
    "\n",
    "                results.append({\n",
    "                    \"change_seq\": change_seq,\n",
    "                    \"in_seq\": in_seq,\n",
    "                    \"pred_seq\": pred_seq,\n",
    "                    \"in_seq_tok\": in_seq_tok,\n",
    "                    \"pred_seq_tok\": pred_seq_tok,\n",
    "                    \"pred_dist\": pred_dist,\n",
    "                })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Define the input sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    \"jewish history is the history of the jewish people, their religion, and culture, as it developed and interacted with other dominant peoples, religions and cultures.\",\n",
    "    \"Black people are always criminal.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = debias(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:   jewish history is the history of the jewish people, their religion, and culture, as it developed and interacted with other dominant peoples, religions and cultures.\n",
      "Changes: jewish history is the history of the jewish people, their religion, and culture, as it developed and interacted with other \u001b[4mdominant\u001b[0m peoples, religions and cultures.\n",
      "Output:  jewish history is the history of the jewish people, their religion, and culture, as it developed and interacted with other peoples, religions and cultures.\n",
      "Bias scores: jewish (0.015) history (0.000) is (0.014) the (0.001) history (0.002) of (0.000) the (0.001) jewish (0.130) people (0.014) , (0.000) their (0.202) religion (0.002) , (0.000) and (0.000) culture (0.000) , (0.000) as (0.000) it (0.000) developed (0.002) and (0.000) interact (0.002) ##ed (0.001) with (0.000) other (0.024) dominant (0.445) peoples (0.002) , (0.000) religions (0.000) and (0.000) cultures (0.000) . (0.000) \n",
      "\n",
      "Input:   black people are always criminal.\n",
      "Changes: black people are \u001b[4malways\u001b[0m \u001b[4mcriminal\u001b[0m.\n",
      "Output:  black people are.\n",
      "Bias scores: black (0.008) people (0.007) are (0.002) always (0.809) criminal (0.137) . (0.000) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    print(\"Input:   {}\".format(result[\"in_seq\"]))\n",
    "    print(\"Changes: {}\".format(result[\"change_seq\"]))\n",
    "    print(\"Output:  {}\".format(result[\"pred_seq\"]))\n",
    "    print(\"Bias scores: \" + (\"{} ({:.3f}) \" * len(result[\"pred_dist\"])).format(\n",
    "        *tuple([\n",
    "            tok\n",
    "            for dist in zip(result[\"in_seq_tok\"].split(), result[\"pred_dist\"])\n",
    "            for tok in dist\n",
    "        ])))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31bfecb55e47aa6114916bebd989923dee775c9fc4561c703a73924e94baf6a2"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
