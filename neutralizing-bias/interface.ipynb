{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interface - *neutralizing-bias*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run in Shell\n",
    "__Concurrent__\n",
    "\n",
    "`python joint/inference.py --bert_encoder --bert_full_embeddings --coverage --debias_checkpoint models/debias_model.ckpt --debias_weight 1.3 --inference_output inference_concurrent/output.txt --no_tok_enrich --pointer_generator --test inference_concurrent/data.txt --working_dir inference_concurrent/`\n",
    "\n",
    "__Modular__\n",
    "\n",
    "`python joint/inference.py --activation_hidden --bert_full_embeddings --checkpoint models/model.ckpt --coverage --debias_weight 1.3 --extra_features_top --inference_output inference_modular/output.txt --pointer_generator --pre_enrich --test inference_modular/data.txt --token_softmax --working_dir inference_modular/`\n",
    "\n",
    "_Optional:_ `--tagging_pretrain_epochs 3 --test_batch_size 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import subprocess\n",
    "import spacy\n",
    "from typing import Tuple, List, Dict, Union\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "\n",
    "NLP = spacy.load(\n",
    "    \"en_core_web_sm\"\n",
    ")  # run \"python -m spacy download en_core_web_sm\" to initially download the model\n",
    "TOKENIZER = BertTokenizer.from_pretrained(\"bert-base-uncased\",\n",
    "                                          cache_dir=\"cache\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_dep(s: str) -> Tuple[str, str]:\n",
    "    \"\"\"Get POS and dependency tags for a given string.\n",
    "    \"\"\"\n",
    "    toks = s.split()\n",
    "\n",
    "    def words_from_toks(toks):\n",
    "        words = []\n",
    "        word_indices = []\n",
    "        for i, tok in enumerate(toks):\n",
    "            if tok.startswith('##'):\n",
    "                words[-1] += tok.replace('##', '')\n",
    "                word_indices[-1].append(i)\n",
    "            else:\n",
    "                words.append(tok)\n",
    "                word_indices.append([i])\n",
    "        return words, word_indices\n",
    "\n",
    "    out_pos, out_dep = [], []\n",
    "    words, word_indices = words_from_toks(toks)\n",
    "    analysis = NLP(\" \".join(words))\n",
    "\n",
    "    if len(analysis) != len(words):\n",
    "        return None, None\n",
    "\n",
    "    for analysis_tok, idx in zip(analysis, word_indices):\n",
    "        out_pos += [analysis_tok.pos_] * len(idx)\n",
    "        out_dep += [analysis_tok.dep_] * len(idx)\n",
    "\n",
    "    assert len(out_pos) == len(out_dep) == len(toks)\n",
    "\n",
    "    return \" \".join(out_pos), \" \".join(out_dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s: str):\n",
    "    \"\"\"BERT-tokenize a given string.\n",
    "    \"\"\"\n",
    "    global TOKENIZER\n",
    "    tok_list = TOKENIZER.tokenize(s.strip())\n",
    "    return \" \".join(tok_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debias(\n",
    "    input: List[str],\n",
    "    concurrent: bool = False,\n",
    ") -> List[Dict[str, Union[str, float]]]:\n",
    "    \"\"\"\n",
    "    Adapter for running the de-biasing script \"inference.py\" on the concurrent and the modular model architecture.\n",
    "\n",
    "    Args:\n",
    "        input - list of sentences to be debiased\n",
    "        concurrent - run concurrent model (if False, run modular model)\n",
    "\n",
    "    Returns:\n",
    "        results - list of output dicts\n",
    "    \"\"\"\n",
    "    assert len(input) > 0\n",
    "\n",
    "    working_dir = \"inference_concurrent\" if concurrent else \"inference_modular\"\n",
    "    inference_output = os.path.join(working_dir, \"output.txt\")\n",
    "    test = os.path.join(working_dir, \"data.txt\")\n",
    "\n",
    "    if os.path.exists(inference_output):\n",
    "        os.remove(inference_output)\n",
    "\n",
    "    if os.path.exists(test):\n",
    "        os.remove(test)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # create input file\n",
    "    with open(test, \"w\") as f:\n",
    "        for s in input:\n",
    "            tok = tokenize(s)\n",
    "            pos, dep = get_pos_dep(tok)\n",
    "            f.write(\"0\\t\" + (tok + \"\\t\") * 2 + (s + \"\\t\") * 2 + pos + \"\\t\" +\n",
    "                    dep + \"\\n\")\n",
    "\n",
    "    # compile command line arguments\n",
    "    args = [\n",
    "        sys.executable, \"joint/inference.py\", \"--bert_full_embeddings\",\n",
    "        \"--coverage\", \"--debias_weight\", \"1.3\", \"--inference_output\",\n",
    "        inference_output, \"--pointer_generator\", \"--test\", test,\n",
    "        \"--working_dir\", working_dir\n",
    "    ]\n",
    "\n",
    "    if concurrent:\n",
    "        args.append(\"--bert_encoder\")\n",
    "        args.append(\"--debias_checkpoint\")\n",
    "        args.append(\"models/debias_model.ckpt\")\n",
    "        args.append(\"--no_tok_enrich\")\n",
    "\n",
    "    else:\n",
    "        args.append(\"--activation_hidden\")\n",
    "        args.append(\"--checkpoint\")\n",
    "        args.append(\"models/model.ckpt\")\n",
    "        args.append(\"--extra_features_top\")\n",
    "        args.append(\"--pre_enrich\")\n",
    "        args.append(\"--test_batch_size\")\n",
    "        args.append(\"1\")\n",
    "        args.append(\"--token_softmax\")\n",
    "\n",
    "    # execute shell script\n",
    "    subprocess.run(\" \".join(args), shell=True)\n",
    "    \n",
    "    ## use for debugging ##\n",
    "    # debug_output = subprocess.run(\" \".join(args),\n",
    "    #                               text=True,\n",
    "    #                               capture_output=True)\n",
    "    # print(debug_output.stderr)\n",
    "    # print(debug_output.stdout)\n",
    "    #######################\n",
    "\n",
    "    # read output\n",
    "    with open(inference_output, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if re.match(r'^IN SEQ:', line):\n",
    "                in_seq_tok = line.split(\"\\t\")[1][3:-2]\n",
    "                in_seq = in_seq_tok.replace(\" ##\", \"\")\n",
    "                in_seq = re.sub(r'\\s([.,;?!\"])', r'\\1', in_seq)\n",
    "\n",
    "            if re.match(r'^PRED SEQ:', line):\n",
    "                pred_seq_tok = line.split(\"\\t\")[1][3:-2]\n",
    "                pred_seq = pred_seq_tok.replace(\" ##\", \"\")\n",
    "                pred_seq = re.sub(r'\\s([.,;?!\"])', r'\\1', pred_seq)\n",
    "\n",
    "            if re.match(r'^PRED DIST:', line):\n",
    "                pred_dist = line.split(\"\\t\")[1]\n",
    "                pred_dist = ast.literal_eval(pred_dist)\n",
    "\n",
    "                # create sequence with underlined changes\n",
    "                change_seq = in_seq_tok.split()\n",
    "                difs = list(\n",
    "                    set(in_seq_tok.split()) - set(pred_seq_tok.split()))\n",
    "\n",
    "                for dif in difs:\n",
    "                    idx = change_seq.index(dif)\n",
    "\n",
    "                    if pred_dist[idx] > 0.1:\n",
    "                        change_seq[idx] = \"\\033[4m{}\\033[0m\".format(\n",
    "                            change_seq[idx])\n",
    "\n",
    "                change_seq = \" \".join(change_seq)\n",
    "                change_seq = change_seq.replace(\" ##\", \"\")\n",
    "                change_seq = re.sub(r'\\s([.,;?!\"])', r'\\1', change_seq)\n",
    "\n",
    "                results.append({\n",
    "                    \"change_seq\": change_seq,\n",
    "                    \"in_seq\": in_seq,\n",
    "                    \"pred_seq\": pred_seq,\n",
    "                    \"in_seq_tok\": in_seq_tok,\n",
    "                    \"pred_seq_tok\": pred_seq_tok,\n",
    "                    \"pred_dist\": pred_dist,\n",
    "                })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Define the input sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    \"jewish history is the history of the jewish people, their religion, and culture, as it developed and interacted with other dominant peoples, religions and cultures.\",\n",
    "    \"Black people are always criminal.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_modular = debias(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change lines 75 and 80 in `joint/inference.py` to toggle GPU/CPU use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_concurrent = debias(inputs, concurrent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results_modular:\n",
    "    print(\"Input:   {}\".format(result[\"in_seq\"]))\n",
    "    print(\"Changes: {}\".format(result[\"change_seq\"]))\n",
    "    print(\"Output:  {}\".format(result[\"pred_seq\"]))\n",
    "    print(\"Bias scores: \" + (\"{} ({:.3f}) \" * len(result[\"pred_dist\"])).format(\n",
    "        *tuple([\n",
    "            tok\n",
    "            for dist in zip(result[\"in_seq_tok\"].split(), result[\"pred_dist\"])\n",
    "            for tok in dist\n",
    "        ])))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results_concurrent:\n",
    "    print(\"Input:   {}\".format(result[\"in_seq\"]))\n",
    "    print(\"Changes: {}\".format(result[\"change_seq\"]))\n",
    "    print(\"Output:  {}\".format(result[\"pred_seq\"]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x: float, start_col: Tuple[int]=(255, 255, 255), end_col: Tuple[int]=(250, 50, 50)) -> str:\n",
    "    \"\"\" Returns the HEX code for a color at position x âˆˆ [0; 1] within a color gradient of start_col and end_col.\n",
    "    \"\"\"\n",
    "    rgb = (\n",
    "        int((1 - x) * start_col[0] + x * end_col[0]),\n",
    "        int((1 - x) * start_col[1] + x * end_col[1]),\n",
    "        int((1 - x) * start_col[2] + x * end_col[2]))\n",
    "    return \"#%02x%02x%02x\" % rgb\n",
    "\n",
    "\n",
    "def print_results(results: List[Dict[str, Union[str, List[float]]]], out_file: str=\"\") -> None:\n",
    "    \"\"\" Prints the result tokens with a highlighted background according to the subjective bias probability.\n",
    "    \"\"\"\n",
    "    from IPython.display import display, HTML\n",
    "\n",
    "    tok_lists = [x[\"in_seq_tok\"] for x in results]\n",
    "    dist_lists = [x[\"pred_dist\"]for x in results]\n",
    "    html_list = []\n",
    "\n",
    "    for toks, dists in zip(tok_lists, dist_lists):\n",
    "        html_string = \"<div style='background-color:white;padding:10px;margin:-8px'>\"\n",
    "\n",
    "        for tok, dist in zip(toks.split(\" \"), dists):\n",
    "            html_string += \"<span style='color:black;background-color: \" + gradient(dist) + \"'>\" + tok + \"</span>\" + \" \"\n",
    "\n",
    "        html_string += \"</div>\"\n",
    "        html_list.append(HTML(html_string))\n",
    "\n",
    "    if len(out_file) > 0:\n",
    "        with open(out_file, \"w\") as f:\n",
    "            f.write(\"\\n\".join(html.data) + \"\\n\")\n",
    "\n",
    "    for html in html_list:\n",
    "        display(html)\n",
    "\n",
    "\n",
    "print_results(results_modular)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6da1900acec40547dccdf6ed82ff53d67318af5ad673ca75f174b699dab6a3b4"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('.venv-nb': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
