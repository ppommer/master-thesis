{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Visualize word embeddings (NLP specialization; t-SNE, Week 2 of Sequence Models)\n",
    "2. De-bias word embeddings (Week 2 of Sequence Models)\n",
    "3. Neural Machine Translation (Week 3 of Sequence Models) - problem: supervised!\n",
    "\n",
    "Bleu Score for validating hypothesis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground for the visualization of word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://towardsdatascience.com/visualizing-word-embedding-with-pca-and-t-sne-961a692509f5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplerepresentations\n",
    "# spacy\n",
    "# torch\n",
    "# transformers\n",
    "# gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations on Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from w2v_utils import read_glove_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 50-dimensional GloVe word vectors\n",
    "words, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: replace with commodity library function (spacy, gensim)\n",
    "\n",
    "def cosine_similarity(u, v):\n",
    "    \"\"\"\n",
    "    Cosine similarity reflects the degree of similarity between u and v\n",
    "        \n",
    "    Arguments:\n",
    "        u -- a word vector of shape (n,)          \n",
    "        v -- a word vector of shape (n,)\n",
    "\n",
    "    Returns:\n",
    "        cosine_similarity -- the cosine similarity between u and v defined by the formula above.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Special case. Consider the case u = [0, 0], v=[0, 0]\n",
    "    if np.all(u == v):\n",
    "        return 1\n",
    "    \n",
    "    # Compute the dot product between u and v (≈1 line)\n",
    "    dot = np.dot(u, v)\n",
    "    # Compute the L2 norm of u (≈1 line)\n",
    "    norm_u = np.sqrt(np.sum(u**2))\n",
    "    \n",
    "    # Compute the L2 norm of v (≈1 line)\n",
    "    norm_v = np.sqrt(np.sum(v**2))\n",
    "    \n",
    "    # Avoid division by 0\n",
    "    if np.isclose(norm_u * norm_v, 0, atol=1e-32):\n",
    "        return 0\n",
    "    \n",
    "    # Compute the cosine similarity defined by formula (1) (≈1 line)\n",
    "    cosine_similarity = dot / (norm_u * norm_v)\n",
    "    \n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0\n",
      "-0.15\n",
      "man-doctor:\t0.71\n",
      "woman-nurse:\t0.72\n",
      "man-woman:\t0.89\n",
      "doctor-nurse:\t0.8\n",
      "man-nurse:\t0.57\n",
      "woman-doctor:\t0.73\n"
     ]
    }
   ],
   "source": [
    "# Load word vectors\n",
    "man = word_to_vec_map[\"man\"]\n",
    "doctor = word_to_vec_map[\"doctor\"]\n",
    "woman = word_to_vec_map[\"woman\"]\n",
    "nurse = word_to_vec_map[\"nurse\"]\n",
    "\n",
    "# Demonstrate gender bias\n",
    "man_to_doctor_as_woman_to_nurse = cosine_similarity(man, doctor) - cosine_similarity(woman, nurse)\n",
    "man_to_nurse_as_woman_to_doctor = cosine_similarity(man, nurse) - cosine_similarity(woman, doctor)\n",
    "\n",
    "# Print results\n",
    "print(round(man_to_doctor_as_woman_to_nurse, 2))\n",
    "print(round(man_to_nurse_as_woman_to_doctor, 2))\n",
    "print(f\"man-doctor:\\t{round(cosine_similarity(man, doctor), 2)}\")\n",
    "print(f\"woman-nurse:\\t{round(cosine_similarity(woman, nurse), 2)}\")\n",
    "print(f\"man-woman:\\t{round(cosine_similarity(man, woman), 2)}\")\n",
    "print(f\"doctor-nurse:\\t{round(cosine_similarity(doctor, nurse), 2)}\")\n",
    "print(f\"man-nurse:\\t{round(cosine_similarity(man, nurse), 2)}\")\n",
    "print(f\"woman-doctor:\\t{round(cosine_similarity(woman, doctor), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_analogy(word_a, word_b, word_c, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Performs the word analogy task: a is to b as c is to ____. \n",
    "    \n",
    "    Arguments:\n",
    "    word_a -- a word, string\n",
    "    word_b -- a word, string\n",
    "    word_c -- a word, string\n",
    "    word_to_vec_map -- dictionary that maps words to their corresponding vectors. \n",
    "    \n",
    "    Returns:\n",
    "    best_word --  the word such that v_b - v_a is close to v_best_word - v_c, as measured by cosine similarity\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert words to lowercase\n",
    "    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n",
    "    \n",
    "    # Get the word embeddings e_a, e_b and e_c (≈1-3 lines)\n",
    "    e_a, e_b, e_c = word_to_vec_map[word_a], word_to_vec_map[word_b], word_to_vec_map[word_c]\n",
    "    \n",
    "    words = word_to_vec_map.keys()\n",
    "    max_cosine_sim = -100              # Initialize max_cosine_sim to a large negative number\n",
    "    best_word = None                   # Initialize best_word with None, it will help keep track of the word to output\n",
    "    \n",
    "    # Loop over the whole word vector set\n",
    "    for w in words:   \n",
    "        # To avoid best_word being one of the input words, skip the input word_c\n",
    "        # Skip word_c from query\n",
    "        if w == word_c:\n",
    "            continue\n",
    "        \n",
    "        # Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c)  (≈1 line)\n",
    "        cosine_sim = cosine_similarity(e_b - e_a, word_to_vec_map[w] - e_c)\n",
    "        \n",
    "        # If the cosine_sim is more than the max_cosine_sim seen so far,\n",
    "            # Then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word (≈3 lines)\n",
    "        if cosine_sim > max_cosine_sim:\n",
    "            max_cosine_sim = cosine_sim\n",
    "            best_word = w\n",
    "        \n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "italy -> italian :: spain -> spanish\n",
      "india -> delhi :: japan -> tokyo\n",
      "man -> woman :: boy -> girl\n",
      "small -> smaller :: large -> smaller\n"
     ]
    }
   ],
   "source": [
    "# Define triads to try analogies\n",
    "triads = [('italy', 'italian', 'spain'), ('india', 'delhi', 'japan'), ('man', 'woman', 'boy'), ('small', 'smaller', 'large')]\n",
    "\n",
    "# Print results\n",
    "for triad in triads:\n",
    "    print ('{} -> {} :: {} -> {}'.format( *triad, complete_analogy(*triad, word_to_vec_map)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debiasing Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.07656667  0.34967667 -0.40057667 -0.03130333  0.0088      0.72586333\n",
      "  0.10256     0.14906333  0.4780662  -0.22850987  0.05957667 -0.68663\n",
      "  0.62210033  0.10395     0.17747667  0.09556867 -0.49258333 -0.17066233\n",
      "  0.46930033  0.02196333  0.28145667  0.50513333  0.17144733  0.40154767\n",
      "  0.24039333  0.1646     -0.17984667  0.24042667  0.05689333 -0.31423\n",
      " -0.10933333  0.26355967  0.06100667 -0.01156405 -0.12236333 -0.188245\n",
      " -0.13215057 -0.068186    0.05624667 -0.29555567 -0.09669533 -0.29559667\n",
      "  0.62465867 -0.40130167  0.03330667 -0.24831667  0.26381667 -0.28738333\n",
      "  0.03020433  0.054106  ]\n"
     ]
    }
   ],
   "source": [
    "# Calculate distance between gender pair woman-man\n",
    "woman_man = word_to_vec_map['woman'] - word_to_vec_map['man']\n",
    "\n",
    "# Calculate distance between gender pair mother-father\n",
    "mother_father = word_to_vec_map['mother'] - word_to_vec_map['father']\n",
    "\n",
    "# Calculate distance between gender pair girl-boy\n",
    "girl_boy = word_to_vec_map['girl'] - word_to_vec_map['boy']\n",
    "\n",
    "# Average over the gender pairs to get a simple representation of gender\n",
    "gender = np.average([woman_man, mother_father, girl_boy], axis=0)\n",
    "\n",
    "# Print gender vector\n",
    "print(gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a94588eda9d64d9e9a351ab8144e55b1fabf5113b54e67dd26a8c27df0381b3"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
