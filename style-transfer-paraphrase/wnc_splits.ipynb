{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WNC Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "source_path = \"../neutralizing-bias/src/bias_data/WNC\"\n",
    "target_path = \"datasets/WNC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(lines: list[str]) -> list[str]:\n",
    "    \"\"\" Removes duplicates from a list of strings while preserving its order. \"\"\"\n",
    "    known_lines = set()\n",
    "    new_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line in known_lines:\n",
    "            continue\n",
    "        new_lines.append(line)\n",
    "        known_lines.add(line)\n",
    "\n",
    "    return new_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. WNC_biased_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = []\n",
    "dev_list = []\n",
    "test_list = []\n",
    "\n",
    "# Add sentences to list\n",
    "with open(os.path.join(source_path, \"biased.word.train\"), \"r\", encoding=\"utf8\") as in_file:\n",
    "    for line in in_file:\n",
    "        sentence = line.split(\"\\t\")[4]\n",
    "        train_list.append(sentence)\n",
    "\n",
    "train_list = remove_duplicates(train_list)\n",
    "\n",
    "# Write sentences to file\n",
    "with open(os.path.join(target_path, \"WNC_biased_word/train.txt\"), \"w\", encoding=\"utf8\") as out_file:\n",
    "    with open(os.path.join(target_path, \"WNC_biased_word/train.label\"), \"w\", encoding=\"utf8\") as label_file:\n",
    "        for sentence in train_list:\n",
    "            out_file.write(\"{}\\n\".format(sentence))\n",
    "            label_file.write(\"{}\\n\".format(\"neutral\"))\n",
    "\n",
    "# Add sentences to list\n",
    "with open(os.path.join(source_path, \"biased.word.dev\"), \"r\", encoding=\"utf8\") as in_file:\n",
    "    for line in in_file:\n",
    "        sentence = line.split(\"\\t\")[4]\n",
    "        dev_list.append(sentence)\n",
    "\n",
    "dev_list = remove_duplicates(dev_list)\n",
    "\n",
    "# Write sentences to file\n",
    "with open(os.path.join(target_path, \"WNC_biased_word/dev.txt\"), \"w\", encoding=\"utf8\") as out_file:\n",
    "    with open(os.path.join(target_path, \"WNC_biased_word/dev.label\"), \"w\", encoding=\"utf8\") as label_file:\n",
    "        for sentence in dev_list:\n",
    "            out_file.write(\"{}\\n\".format(sentence))\n",
    "            label_file.write(\"{}\\n\".format(\"neutral\"))\n",
    "\n",
    "# Add sentences to list\n",
    "with open(os.path.join(source_path, \"biased.word.test\"), \"r\", encoding=\"utf8\") as in_file:\n",
    "    for line in in_file:\n",
    "        sentence = line.split(\"\\t\")[4]\n",
    "        test_list.append(sentence)\n",
    "\n",
    "test_list = remove_duplicates(test_list)\n",
    "\n",
    "# Write sentences to file\n",
    "with open(os.path.join(target_path, \"WNC_biased_word/test.txt\"), \"w\", encoding=\"utf8\") as out_file:\n",
    "    with open(os.path.join(target_path, \"WNC_biased_word/test.label\"), \"w\", encoding=\"utf8\") as label_file:\n",
    "        for sentence in test_list:\n",
    "            out_file.write(\"{}\\n\".format(sentence))\n",
    "            label_file.write(\"{}\\n\".format(\"neutral\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. WNC_biased_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "sentences = []\n",
    "train_list = []\n",
    "dev_list = []\n",
    "test_list = []\n",
    "\n",
    "# Read sentences\n",
    "with open(os.path.join(source_path, \"biased.full\"), \"r\", encoding=\"utf8\") as in_file:\n",
    "    for line in in_file:\n",
    "        sentence = line.split(\"\\t\")[4]\n",
    "        sentences.append(sentence)\n",
    "\n",
    "# Make sure that all sentences of biased.word.test are in the test set\n",
    "with open(os.path.join(source_path, \"biased.word.test\"), \"r\", encoding=\"utf8\") as in_file:\n",
    "    for line in in_file:\n",
    "        sentence = line.split(\"\\t\")[4]\n",
    "        sentences.append(sentence)\n",
    "        test_list.append(sentence)\n",
    "\n",
    "sentences = remove_duplicates(sentences)\n",
    "num_sentences = len(sentences)\n",
    "random.shuffle(sentences)\n",
    "\n",
    "# Add sentences to lists\n",
    "for sentence in tqdm(sentences):\n",
    "    if sentence in test_list:\n",
    "        continue\n",
    "    \n",
    "    if len(test_list) < 0.05 * num_sentences:\n",
    "        test_list.append(sentence)\n",
    "    elif len(dev_list) < 0.05 * num_sentences:\n",
    "        dev_list.append(sentence)\n",
    "    else:\n",
    "        train_list.append(sentence)\n",
    "\n",
    "# Write sentences to files\n",
    "with open(os.path.join(target_path, \"WNC_biased_full/train.txt\"), \"w\", encoding=\"utf8\") as train_file:\n",
    "    with open(os.path.join(target_path, \"WNC_biased_full/train.label\"), \"w\", encoding=\"utf8\") as train_label_file:\n",
    "        for sentence in train_list:\n",
    "            train_file.write(\"{}\\n\".format(sentence))\n",
    "            train_label_file.write(\"{}\\n\".format(\"neutral\"))\n",
    "\n",
    "with open(os.path.join(target_path, \"WNC_biased_full/dev.txt\"), \"w\", encoding=\"utf8\") as dev_file:\n",
    "    with open(os.path.join(target_path, \"WNC_biased_full/dev.label\"), \"w\", encoding=\"utf8\") as dev_label_file:\n",
    "        for sentence in dev_list:\n",
    "            dev_file.write(\"{}\\n\".format(sentence))\n",
    "            dev_label_file.write(\"{}\\n\".format(\"neutral\"))\n",
    "                \n",
    "with open(os.path.join(target_path, \"WNC_biased_full/test.txt\"), \"w\", encoding=\"utf8\") as test_file:\n",
    "    with open(os.path.join(target_path, \"WNC_biased_full/test.label\"), \"w\", encoding=\"utf8\") as test_label_file:\n",
    "        for sentence in test_list:\n",
    "            test_file.write(\"{}\\n\".format(sentence))\n",
    "            test_label_file.write(\"{}\\n\".format(\"neutral\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. WNC_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "sentences = []\n",
    "train_list = []\n",
    "dev_list = []\n",
    "test_list = []\n",
    "\n",
    "# Read sentences\n",
    "with open(os.path.join(source_path, \"biased.full\"), \"r\", encoding=\"utf8\") as in_file:\n",
    "    for line in in_file:\n",
    "        sentence = line.split(\"\\t\")[3]\n",
    "        sentences.append(sentence)\n",
    "\n",
    "with open(os.path.join(source_path, \"neutral\"), \"r\", encoding=\"utf8\") as in_file:\n",
    "    for line in in_file:\n",
    "        sentence = line.split(\"\\t\")[3]\n",
    "        sentences.append(sentence)\n",
    "\n",
    "# Make sure that all sentences of biased.word.test are in the test set\n",
    "with open(os.path.join(source_path, \"biased.word.test\"), \"r\", encoding=\"utf8\") as in_file:\n",
    "    for line in in_file:\n",
    "        sentence = line.split(\"\\t\")[3]\n",
    "        sentences.append(sentence)\n",
    "        test_list.append(sentence)\n",
    "\n",
    "sentences = remove_duplicates(sentences)\n",
    "num_sentences = len(sentences)\n",
    "random.shuffle(sentences)\n",
    "\n",
    "# Add sentences to lists\n",
    "for sentence in tqdm(sentences):\n",
    "    if sentence in test_list:\n",
    "        continue\n",
    "\n",
    "    if len(test_list) < 0.05 * num_sentences:\n",
    "        test_list.append(sentence)\n",
    "    elif len(dev_list) < 0.05 * num_sentences:\n",
    "        dev_list.append(sentence)\n",
    "    else:\n",
    "        train_list.append(sentence)\n",
    "\n",
    "# Write sentences to files\n",
    "with open(os.path.join(target_path, \"WNC_large/train.txt\"), \"w\", encoding=\"utf8\") as train_file:\n",
    "    with open(os.path.join(target_path, \"WNC_large/train.label\"), \"w\", encoding=\"utf8\") as train_label_file:\n",
    "        for sentence in train_list:\n",
    "            train_file.write(\"{}\\n\".format(sentence))\n",
    "            train_label_file.write(\"{}\\n\".format(\"neutral\"))\n",
    "\n",
    "with open(os.path.join(target_path, \"WNC_large/dev.txt\"), \"w\", encoding=\"utf8\") as dev_file:\n",
    "    with open(os.path.join(target_path, \"WNC_large/dev.label\"), \"w\", encoding=\"utf8\") as dev_label_file:\n",
    "        for sentence in dev_list:\n",
    "            dev_file.write(\"{}\\n\".format(sentence))\n",
    "            dev_label_file.write(\"{}\\n\".format(\"neutral\"))\n",
    "\n",
    "with open(os.path.join(target_path, \"WNC_large/test.txt\"), \"w\", encoding=\"utf8\") as test_file:\n",
    "    with open(os.path.join(target_path, \"WNC_large/test.label\"), \"w\", encoding=\"utf8\") as test_label_file:\n",
    "        for sentence in test_list:\n",
    "            test_file.write(\"{}\\n\".format(sentence))\n",
    "            test_label_file.write(\"{}\\n\".format(\"neutral\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_map = {}\n",
    "\n",
    "with open(os.path.join(source_path, \"biased.full\"), \"r\", encoding=\"utf8\") as in_file:\n",
    "    for line in in_file:\n",
    "        bias = line.split(\"\\t\")[3]\n",
    "        neutral = line.split(\"\\t\")[4]\n",
    "        target_map[neutral] = bias\n",
    "\n",
    "with open(os.path.join(target_path, \"WNC_biased_full/test.txt\"), \"r\", encoding=\"utf8\") as test_file:\n",
    "    with open(os.path.join(target_path, \"targets.txt\"), \"w\", encoding=\"utf8\") as targets:\n",
    "        for line in test_file:\n",
    "            neutral = line.strip()\n",
    "            targets.write(\"\\t\".join([target_map[neutral], neutral]) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31bfecb55e47aa6114916bebd989923dee775c9fc4561c703a73924e94baf6a2"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
